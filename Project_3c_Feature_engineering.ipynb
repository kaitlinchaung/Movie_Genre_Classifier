{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "import textwrap\n",
    "import pickle\n",
    "import en_core_web_sm\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from scipy import spatial\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from scipy import spatial\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering of doc2vec vectors and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    Cleans text for doc2vec\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Returns tokenized text\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    \"\"\"\n",
    "    Returns feature vector, given model\n",
    "    \"\"\"\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_tagged, test_tagged, dm):\n",
    "    \"\"\"\n",
    "    Build model and vocabulary\n",
    "    \"\"\"\n",
    "    if dm == 'dbow':\n",
    "        model = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "        model.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    elif dm == 'dmm':\n",
    "        model = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "        model.build_vocab([x for x in tqdm(train_tagged.values)])  \n",
    "    \"\"\"\n",
    "    Train model\n",
    "    \"\"\"\n",
    "    for epoch in range(30):\n",
    "        model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model.alpha -= 0.002\n",
    "        model.min_alpha = model.alpha\n",
    "    \"\"\"\n",
    "    Build feature vectors\n",
    "    \"\"\"\n",
    "    y_train, X_train = vec_for_learning(model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(model, test_tagged)\n",
    "    \"\"\"\n",
    "    Return feature vectors\n",
    "    \"\"\"\n",
    "    return X_train, X_test, y_train, y_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_dist(model, begin_text,end_text):\n",
    "    \"\"\"\n",
    "    Get cosine distance between two feature vectors, given a doc2vec model\n",
    "    \"\"\"\n",
    "    begin = model.infer_vector(begin_text.split())\n",
    "    end = model.infer_vector(end_text.split())\n",
    "    return spatial.distance.cosine(begin,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beginning(text):\n",
    "    \"\"\"\n",
    "    Returns first third of text\n",
    "    \"\"\"\n",
    "    divis = int(len(text)/3)\n",
    "    beginning = list(map(''.join, zip(*[iter(text)]*divis)))[0]\n",
    "    return beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ending(text):\n",
    "    \"\"\"\n",
    "    Returns last third of text\n",
    "    \"\"\"\n",
    "    divis = int(len(text)/3)\n",
    "    ending = list(map(''.join, zip(*[iter(text)]*divis)))[2]\n",
    "    return ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_diff(begin, end):\n",
    "    \"\"\"\n",
    "    Get difference in seniment polarity from beginning and end\n",
    "    \"\"\"\n",
    "    begin_sent = TextBlob(begin)\n",
    "    end_sent = TextBlob(end)\n",
    "    return (float(begin_sent.sentiment[0]) - float(end_sent.sentiment[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_df(indices, feature_vect):\n",
    "    \"\"\"\n",
    "    Generate final feature df with flattened feature vect and split script features\n",
    "    \"\"\"\n",
    "    indexed_feature_vect = pd.concat([indices, pd.Series(feature_vect)], axis=1).set_index(0)\n",
    "    merged_df = pd.merge(indexed_feature_vect,split_script,left_index=True,right_index=True)[[1,'cos_dist','log_cos_dist','sentiment_diff']]\n",
    "    merged_df = pd.DataFrame(merged_df.to_records())\n",
    "    merged_df.columns = ['index','feature_vect','cos_dist','log_cos_dist','sentiment_diff']\n",
    "    merged_df = merged_df.drop('index',axis=1)\n",
    "    merged_df['feature_list'] = merged_df.feature_vect.tolist()\n",
    "    final_df = pd.concat([merged_df[['cos_dist','log_cos_dist','sentiment_diff']],merged_df.feature_list.apply(pd.Series)], axis=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    '''\n",
    "    Returns stemmed and lemmatized text\n",
    "    '''\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Returns text, with names and stop words filtered out\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    result = []\n",
    "    doc = nlp(text)\n",
    "    names = list(dict.fromkeys([X.text.lower() for X in doc.ents if X.label_ == 'PERSON']))\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 2 and token not in names:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_model(bow_corpus, num_topics, dictionary, processed_docs):\n",
    "    \"\"\"\n",
    "    Returns LDA model and model metrics\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.LdaMulticore(train_bow_corpus, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word=train_dict,\n",
    "                                       passes=10,\n",
    "                                       eta = 0.01,\n",
    "                                       workers=13)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=train_processed_docs, dictionary=train_dict, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    perplexity = lda_model.log_perplexity(train_bow_corpus)\n",
    "    return(lda_model, coherence, perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(X, lda_model, bow_corpus, num_topics):\n",
    "    \"\"\"\n",
    "    Returns feature vectors for each document\n",
    "    \"\"\"\n",
    "    vecs = []\n",
    "    for i in range(len(X)):\n",
    "        top_topics = lda_model.get_document_topics(bow_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(num_topics)]\n",
    "        vecs.append(topic_vec)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(df):\n",
    "    \"\"\"\n",
    "    Return bigrams from test data\n",
    "    \"\"\"\n",
    "    df['text'] = strip_newline(df.text)\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_moviedataset.csv')\n",
    "\n",
    "data = data[['Scripts','new_genres']]\n",
    "data.columns = ['Scripts','Genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Scripts = data.Scripts.apply(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into test and train, and save index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data.Genres)\n",
    "train_index = pd.Series(train.index)\n",
    "test_index = pd.Series(test.index)\n",
    "\n",
    "with open('train_index.pickle', 'wb') as f:\n",
    "    pickle.dump(train_index, f)\n",
    "with open('test_index.pickle', 'wb') as f:\n",
    "    pickle.dump(test_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag data for doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_scripts = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Scripts']), tags=[r.Genres]), axis=1)\n",
    "test_tagged_scripts = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Scripts']), tags=[r.Genres]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model and extract feature vectors.\n",
    "\n",
    "(Notebook default is to load objects from pickle, due to long object generation time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640/2640 [00:00<00:00, 1653668.24it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2047893.94it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1726643.16it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1884759.58it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1939902.34it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1578244.38it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1682565.35it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2002344.04it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2025418.43it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1903224.92it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1936509.72it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2009247.43it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1759009.14it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2020244.95it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1170874.76it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1241502.70it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1998729.70it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1719938.27it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1922055.64it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1726912.44it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1903224.92it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1978728.12it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2058554.11it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1887973.16it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1925397.77it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1737480.40it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2083734.02it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1908473.38it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1742675.88it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 2017300.52it/s]\n",
      "100%|██████████| 2640/2640 [00:00<00:00, 1819116.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#X_train_scripts_dmm,X_test_scripts_dmm,y_train_scripts_dmm,y_test_scripts_dmm, model_scripts_dmm = get_feature_vector(train_tagged_scripts, test_tagged_scripts, \"dmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scripts_dmm = pickle.load(open( \"X_train_dmm.pickle\", \"rb\"))\n",
    "y_train_scripts_dmm = pickle.load(open( \"y_train_dmm.pickle\", \"rb\"))\n",
    "X_test_scripts_dmm = pickle.load(open( \"X_test_dmm.pickle\", \"rb\"))\n",
    "y_test_scripts_dmm = pickle.load(open( \"y_test_dmm.pickle\", \"rb\"))\n",
    "\n",
    "dmm_model = pickle.load(open('model_scripts_dmm.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate topic change as feature vector cosine distance from movie's beginning(first third) to movie's ending(last third)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Scripts = data.Scripts.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_script = pd.DataFrame()\n",
    "\n",
    "split_script['beginning'] = data.Scripts.map(lambda x: get_beginning(x))\n",
    "split_script['ending'] = data.Scripts.map(lambda x: get_ending(x))\n",
    "split_script['cos_dist']=split_script.apply(lambda x: get_cosine_dist(dmm_model,x.beginning,x.ending),axis=1)\n",
    "split_script['log_cos_dist'] = np.log(split_script.cos_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sentiment change as difference of sentiment polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_script['sentiment_diff'] = split_script.apply(lambda x: get_sentiment_diff(x.beginning, x.ending),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge split script metrics with train and test feature vectors, to form final test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_script = split_script.drop(['beginning','ending'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_features_df(train_index, X_train_scripts_dmm)\n",
    "X_test = make_features_df(test_index, X_test_scripts_dmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign targets for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_scripts_dmm\n",
    "y_test = y_test_scripts_dmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open('X_train.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open('X_test.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open('y_train.pickle', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open('y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m53",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m53"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
